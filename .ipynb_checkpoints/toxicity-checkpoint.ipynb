{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    './datas/crawl-300d-2M.vec',\n",
    "    './datas/glove.840B.300d.txt'\n",
    "]\n",
    "\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "\n",
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "\n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./datas/train.csv')\n",
    "test_df = pd.read_csv('./datas/test.csv')\n",
    "\n",
    "x_train = train_df[TEXT_COLUMN].astype(str)\n",
    "y_train = train_df[TARGET_COLUMN].values\n",
    "y_aux_train = train_df[AUX_COLUMNS].values\n",
    "x_test = test_df[TEXT_COLUMN].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n",
    "    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n",
    "\n",
    "tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.ones(len(x_train), dtype=np.float32)\n",
    "sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "sample_weights /= sample_weights.mean()\n",
    "\n",
    "embedding_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
